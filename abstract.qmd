---
title: "Künstliche Intelligenz als Werkzeug - Nutzendenanforderungen an KI-Systeme zur Detektion von Misinformation auf Social Media"
author: 
  - Fynn Luyken
  - Jorin Bahns
  - David Piekarski
  - Johannes Rehrl
  - Nathaniel Roumiantsev
date: today
format:
  html:
    toc: true
    number-sections: false
  pdf:
    papersize: a4
    fontsize: 12pt
    toc: true
    number-sections: false
    citation-location: none
bibliography: Literatur.bib
csl: apa.csl

output-file: abstract
---

**GitHub Repository** https://github.com/nat1vecode/SMNF-GruppeB1

**Für die Abgabe aktueller GitHub Hash:** 20208e41a087d59c2922c078d10c81a0f527c822

## Code of Conduct

Im Rahmen dieser wissenschaftlichen Arbeit verpflichten sich alle beteiligten Personen zur Einhaltung folgender Verhaltensgrundsätze:

**Wissenschaftliche Integrität**

Alle Forschenden verpflichten sich zur Einhaltung guter wissenschaftlicher Praxis, insbesondere zur Ehrlichkeit in der Darstellung von Daten, Ergebnissen und Methoden. Plagiate, Datenfälschung und -manipulation werden strikt abgelehnt.

**Respektvolle Zusammenarbeit**

Die Zusammenarbeit im Team basiert auf gegenseitigem Respekt, Fairness und Gleichberechtigung. Diskriminierung, Belästigung oder andere Formen unangemessenen Verhaltens werden nicht toleriert. Vereinbarte und verpflichtende Termine werden zuverlässig eingehalten oder frühzeitig und begründet kommuniziert.

**Faire Aufteilung der Arbeitslast**

Jegliche Arbeit wird gleichmäßig auf alle Gruppenmitglieder aufgeteilt. Sollte sich jemand benachteiligt fühlen, kann dieser dies jederzeit kommunizieren und wird entsprechend berücksichtigt.

**Vermeidung von Interessenkonflikten & Berücksichtigung von Feedback**

Potenzielle Interessenkonflikte werden frühzeitig offengelegt und transparent kommuniziert, um die Unabhängigkeit und Objektivität der Forschung zu gewährleisten. Konstruktives Feedback wird jederzeit mit Wertschätzung aufgenommen, geprüft und entsprechend umgesetzt.

**Vertraulichkeit**

Vertrauliche Informationen, insbesondere personenbezogene Daten oder nicht veröffentlichte Forschungsergebnisse, werden mit der gebotenen Sorgfalt behandelt. Weiterhin verpflichten sich alle Gruppenmitglieder dazu, sämtliche im Zuge des Moduls SMNF (PY1802) erhobenen vertraulichen Daten nicht außerhalb des Moduls zu verbreiten.

**Umgang mit Künstlicher Intelligenz (KI)**

Künstliche Intelligenz darf zur Unterstützung bei Recherche, Analyse oder Textentwurf eingesetzt werden, muss jedoch in angemessener Form kenntlich gemacht werden. KI-Tools werden nicht als Quelle zitiert oder verwendet, sondern dienen ausschließlich der Hilfestellung im Arbeitsprozess.

## 1. Einleitung

Die Anzahl an Misinformationen auf Social Media nimmt stetig zu, das zeigt unter anderem auch eine Studie der Vodafone-Stiftung aus dem Jahr 2020 (vgl. https://www.vodafone-stiftung.de/wp-content/uploads/2020/12/Studie-Vodafone-Stiftung-Umgang-mit-Falschnachrichten.pdf). Ein fehlendes Erkennen dieser Misinformationen birgt allerdings große gesellschaftliche Gefahren, besonders hinsichtlich der politischen Stabilität einer Demokratie. So kann sie unter anderem dazu beitragen, Wahlen gezielt zu manipulieren oder bewusst Unmut in der Bevölkerung auszulösen. Beispiele, wie das der vergangenen Bundestagswahl im Frühjahr 2025, als seitens einer pro-russischen Kampagne gezielt gefälschte Videos in den sozialen Medien verbreitet wurden, sind längst kein Einzelfall mehr (vgl. https://www.tagesschau.de/inland/bundestagswahl/desinformation-russland-briefwahl-100.html). In Anbetracht dessen sehen wir in der Tatsache, dass sich laut einer PISA-Studie aus dem Jahr 2022 nur etwa 47 Prozent der Befragten sicher waren, die Qualität von Informationen im Internet problemlos beurteilen zu können, großen Handlungsbedarf.

Ein möglicher Lösungsansatz könnte die Zuhilfenahme von Künstlicher Intelligenz sein. Schon jetzt finden KI-Systeme in Form von LLMs (Large Language Models) im Alltag vieler Menschen immer häufiger Verwendung. In unserem Fall soll es um KI-Systeme gehen, die den Menschen bei der Erkennung von Misinformationen unterstützen. Da die alltägliche Verwendung von KI in der Gesellschaft jedoch erst seit wenigen Jahren Bestand hat, wurden viele Aspekte der Schnittstelle zwischen Nutzenden und KI noch nicht ausreichend erforscht. Insbesondere Fragen zur Vertrauenswürdigkeit, Nachvollziehbarkeit und Akzeptanz sind beispielsweise noch weitgehend unbeantwortet. Im Idealfall könnten die Ergebnisse unserer Untersuchung daher zukünftig auch der allgemeinen Nutzendenforschung zu Künstlicher Intelligenz dienen.

Im Rahmen dieses Papers wurden deshalb folgende Forschungsfragen untersucht: 1. Was sind Nutzendenanforderungen an KI als Werkzeug zur Unterstützung bei der Detektion von Misinformation auf Social Media? 2. Wie gehen Personen in diesem Kontext mit unterschiedlichen Feedback-Arten um?

Hierzu wurde mittels Interviewen der Probanden zunächst auf eine qualitative Datenerhebung gesetzt, um ein erstes Stimmungsbild einholen zu können und Hinweise darüber zu erhalten, welche Aspekte entscheidend zu untersuchen sind. Anschließend erfolgte eine quantitative Datenerhebung in Form einer Online-Umfrage. Dabei wurden personenbezogene Daten in einem Fragebogen erhoben, bevor die Probanden zwei mal jeweils zehn Posts richtig nach ihrem Wahrheitsgehalt einordnen sollten, zunächst ohne weitere Hilfe, anschließend mithilfe einer KI.

Ziel dieser Untersuchung war es, Informationen darüber zu erhalten, welche Voraussetzungen eine KI erfüllen muss, um von Nutzenden als Hilfsmittel bei der Erkennung von Misinformationen zum Einsatz kommen zu können. Weiterführend sollten uns diese Untersuchungen helfen, besser zu verstehen, ob und inwiefern KI-Systeme überhaupt dazu geeignet sind, und wie die Umsetzung jener Systeme konkret aussehen müsste.

## 2. Literaturübersicht

Des Weiteren erkennt das Paper "Cognitive AI for Mitigation of Misinformation in Online Social Networks" [@indu2022] die Relevanz menschlicher Kognitionspsychologie für die Analyse von (Mis-)Informationen, und fokussiert sich auf die Kombination dessen mit Künstlicher Intelligenz. Dabei wird darauf abgezielt, den Menschen bei der Beurteilung von Informationen in den sozialen Netzwerken zu unterstützen.

Das Paper „Zero-trust management using AI: Untrusting the trusted accounts in social media?“ [@fottouh2023] hebt hervor, dass das bloße Vertrauen in verifizierte Accounts nicht ausreicht, um die Qualität und Vertrauenswürdigkeit von Informationen auf Social Media sicherzustellen. Die Autoren regen dazu an, über die reine Identifikation von Fake-Accounts hinauszugehen und auch die Inhalte verifizierter Accounts zu analysieren. Im Hinblick auf unsere Forschungsfrage beschäftigt sich diese Arbeit mit der Notwendigkeit einer KI, welche nicht nur visuelle Merkmale (wie Verifizierung) berücksichtigt, sondern auch Informationen inhaltlich auf ihre Vertrauenswürdigkeit bewertet.

Das Paper „Using a Chatbot to Combat Misinformation: Exploring Gratifications, Chatbot Satisfaction and Engagement, and Relationship Quality Using a Chatbot to Combat Misinformation: Exploring Gratifications, Chatbot Satisfaction and Engagement, and Relationship Quality“ [@cheng2024] untersucht auf Basis der Uses-and-Gratifications-Theorie, die Nutzung des von der Weltgesundheitsorganisation (WHO) entwickelten Chatbots „Florence“, welcher entwickelt wurde um Informationen zu COVID-19 zu verbreiten. Bei einer Umfrage mit 591 Teilnehmern konnten Erkenntnisse gesammelt werden, das unter anderem technische Modernität (Modality), persönliche Unterstützung (Agency), Interaktivität (Interaction) und einfache Navigation (Navigability) die Nutzerzufriedenheit positiv beeinflussen. Im Gegenzug hemmen durch den Nutzer wahrgenommene Datenschutzrisiken die Zufriedenheit. Die Studie liefert dahingehend wertvolle Implikationen für die Gestaltung eines Chatbots, zur Bekämpfung von Fehlinformationen auf Sozialen Netzwerken.

Abschließend zeigt die Studie „Exploring the Use of Personalized AI for Identifying Misinformation on Social Media“ [@jahanbakhsh2023], wie personalisierte KI-Systeme die individuellen Einschätzungen der Nutzenden lernen und darauf basierend weitere Inhalte vorbewerten können. Interessant ist dabei die Erkenntnis, dass solche Systeme die Nutzermeinung beeinflussen können – besonders dann, wenn keine Begründung für die KI-Vorhersage gegeben wird. Das zeigt vor allem, dass KI ein zentrales Nutzendenbedürfnis darstellt: Nur wenn die Funktionsweise nachvollziehbar bleibt, kann eine nachhaltige Vertrauensbeziehung zwischen Nutzenden und System entstehen.

## 3. Methode

### Qualitative Methode

Unser Forschungsprojekt behandelt ein komplexes Themengebiet, das vor allem eine große soziale Komponente beeinhaltet. Mithilfe der qualitativen Methode können wir herausfinden, welche Erfahrungen Menschen mit KI gemacht haben und wie sie darüber denken. Insbesondere für die Nutzendenanforderungen ist es hilfreich, konkrete Vorstellungen zur Umsetzung einer KI zur Detektion von Misinformationen genannt zu bekommen.

Ergänzend hierzu bietet es sich nun an, quantitative Ansätze zur weiteren Forschung zu verwenden, die auf den Antworten aus der qualitativen Forschung basieren.

Wir haben uns für 2 Teilnehmende entschieden, die in unserem Studiengang sind und somit möglicherweise entsprechende Erfahrungen zu KI haben. Alle Teilnehmenden sind unmittelbar durch Gespräche auf dem Universitätsgelände rekrutiert worden. Dabei haben wir das eine Interview persönlich und das andere online über Discord. Das persönliche Interview wurde mit einem Smartphonemikrofon und das Online-Interview durch die Software OBS aufgenommen. Zur Transkiption haben wir Better-Whisper in einem lokalen Verzeichnis verwendet.

Für die Analyse haben wir uns für die Bottom-Up-Methode entschieden. Somit haben wir gemeinsam als Gruppe beide Transkriptionen gesichtet und jede Antwort separat, deduktiv kodiert und fortführend alle Codes zusammengefasst und entsprechende große Themen entwickelt. Wir haben keine besondere Software zur Analyse verwendet.

### Quantitative Methode

#### Ablauf der Datenerhebung

Für unsere quantitative Erhebung begann die Datenerhebung am Samstag, den 31.05.2025, und endete am Montag, den 09.06.2025. Zur Teilnahme an der Erhebung wurde den Probanden ein Link zur Verfügung gestellt, über den sie eigenständig teilnehmen konnten. Wir entschieden uns für ein Mixed Design, also eine Kombination aus einem Between-Subjects-Design und einem Within-Subjects-Design (@fig-ablaufdiagramm). Zur Untersuchung des Effekts von KI als Unterstützung bei der Detektion von Misinformationen sollten die Teilnehmenden zunächst Beiträge ohne KI bewerten und anschließend mit KI (Within-Subjects-Design). Um außerdem Unterschiede zwischen evaluativer und empfehlender KI in Bezug auf Workload, Genauigkeit etc. festzustellen, wurden die Teilnehmenden im zweiten Teil zufällig einer der beiden KI-Varianten zugeordnet. Jeweils die Hälfte der Antworten bezieht sich auf eine der beiden KI-Varianten (@fig-ablaufdiagramm).

#### Beschreibung der Teilnehmenden

Zum Abschluss unserer Erhebung haben 17 Teilnehmende unsere Studie vollständig abgeschlossen. Primär haben wir die Erhebung mit Studierenden und Personen im Alter zwischen 18 und 30 Jahren geteilt. Diese Zielgruppe wählten wir bewusst aus, da sie häufig viel Zeit auf sozialen Medien verbringen und im Rahmen von Studium oder Arbeit regelmäßig mit Recherche konfrontiert sind. Dadurch sind sie besonders häufig Misinformationen ausgesetzt. Dies führt dazu, dass sie ein aktuelleres Problembewusstsein im Umgang mit Misinformationen haben, was eine erhöhte Motivation zur Nutzung des Systems sowie ein besseres Verständnis für Risiken und Relevanz begünstigt. Ihre Anforderungen sind somit besonders aussagekräftig für die Gestaltung zukünftiger Systeme.

Gleichzeitig haben viele Teilnehmende bereits privat oder beruflich Erfahrungen mit KI-Tools gesammelt, wodurch ihre Nutzung und Bewertung des KI-Systems in unserer Erhebung realitätsnäher und reflektierter ausfallen könnte.

Um jedoch auch mögliche Unterschiede in den Ergebnissen zwischen verschiedenen Altersgruppen feststellen zu können, waren wir bemüht, zusätzlich Teilnehmende außerhalb des oben genannten Altersspektrum zu rekrutieren.

Ausschlusskriterien für Teilnehmer unserer Erhebung waren, dass sie minderjährig sind und aktuell die Veranstalltung SMNF besuchen.

#### Beschreibung der gewählten Erhebungsmethode

Wir haben wir uns bei der Erhebungsmethode für eine quantitative Methode entschieden. Hierzu wurden im Pre-Test-, Post-Baseline- und Post-Test-Fragebogen Skalen verwendet. Bei der Klassifizierung der Posts wurden je Post messbare Variablen sowie die Entscheidungssicherheit des Nutzers über eine Skala aufgenommen. Unter anderem wird die Technik-Affinität mittels einer ATI-Skala gemessen, das Informationsbewusstsein mithilfe einer SIPA-Skala und die Usability durch eine System Usability Scale.

#### Geplante Analysen

Mittels eines t-Tests soll untersucht werden, inwiefern der Einsatz von KI-Systemen die Entscheidungssicherheit bei der Bewertung von Misinformationen beeinflusst. Als abhängige Variable für den t-Test ist somit die Entscheidungssicherheit, die im Rahmen der Post-Klassifizierung ordinal auf einer sechsstufigen Skala erfasst wird. Verglichen werden die Mittelwerte der Baseline-Gruppe (ohne KI-Unterstützung) mit jenen der KI-Empfehlungsgruppe. Wir erwarten hierbei, dass die KI die Entscheidungssicherheit insgesamt erhöht, weshalb ein gerichteter t-Test geplant ist. Wir halten die Analyse für relevant, da die Entscheidungssicherheit der Nutzenden bei der Beurteilung von Misinformationen wesentlich zu Akzeptanz und Effektivität von KI-Systemen beiträgt.

Des Weiteren soll bestimmt werden, ob eine Korrelation zwischen der Vorerfahrung mit KI seitens der Nutzenden mit der Bearbeitungszeit bei der Entscheidung besteht. Die zu untersuchenden Variablen sind hierbei „Vorwissen KI“, das im Rahmen des Pre-Test-Fragebogens auf einer Skala von 1-5 erfasst wird, sowie die in Sekunden bzw. Millisekunden gemessene „Bearbeitungszeit“ aus der Post-Klassifizierung. Unsere Vermutung hierzu ist, dass Teilnehmende die eine größere Vorerfahrung mit KI angeben, eine geringere Bearbeitungszeit benötigen. Dies würde einer negativen Korrelation entsprechen. Von den Ergebnissen dieser Analyse erhoffen wir uns Erkenntnisse über die Auswirkungen bestehender bzw. fehlender Vorerfahrungen mit KI im Zuge der effizienten Nutzung von KI-gestützten Entscheidungssystemen.

::: {#fig-ablaufdiagramm .figure .quarto-figure}
```{mermaid}
%%| fig-responsive: true
%%| label: Ablaufdiagramm
%%| fig-cap: Ablaufdiagramm der quantitativen Studie
%%| fig-width: 6.5
flowchart LR
 subgraph BASELINE["Phase 1: Ohne KI"]
    direction TB
        n4["Post-Baseline-Fragebogen"]
        n3["Baseline Klassifizierung ohne KI"]
  end
 subgraph KI_PHASE["Phase 2: Mit evaluativer KI"]
    direction TB
        n5["Klassifizierung mit KI"]
        n6["Post-Test-Fragebogen"]
  end
 subgraph KI_PHASE2["Phase 2: Mit empfehlender KI"]
    direction TB
        n12["Klassifizierung mit KI"]
        n13["Post-Test-Fragebogen <br>"]
  end
    n11["Start"] --> n1["Probandeninformationen"]
    n1 --> n2["Pre-Test-Fragebogen"]
    n2 --> n3
    n3 --> n4
    n5 --> n6
    n6 --> n7["Verabschiedung & Dank"]
    n7 --> n8["VP-Stunden-Umfrage"]
    n8 --> n9["Ende"]
    n12 --> n13
    n13 --> n7
    n4 --> n14["Randomization"]
    n14 --> n5
    n14 --> n12

    style n3 color:#000000
    style n5 color:#000000
    style n6 color:#000000
    style n12 color:#000000
    style n13 color:#000000
    style n11 color:#000000
    style n1 color:#000000
    style n2 color:#000000
    style n7 color:#000000
    style n8 color:#000000
    style n9 color:#000000
    style n14 color:#000000
    style KI_PHASE2 fill:transparent
    style KI_PHASE fill:transparent
    style BASELINE fill:transparent
```
:::

## 4. Ergebnisse

### Qualitative Ergebnisse

Bei unserer qualitativen Erhebung haben wir zwei Interviews geführt und gemeinsame analysiert. Besonders relevant bei unserer Stichprobe ist, dass beide Interviewpartner in einem jungen Alter sind, dementsprechend regelmäßig Social-Media nutzen und somit schon häufig in Kontakt mit Misinformationen waren.

Im Zuge der Analyse konnten wir einige Themen herausarbeiten, die im Folgenden erläutert werden sollen.

Zunächst ergibt sich die grundsätzliche Frage nach der Verantwortlichkeit und Regulierung für und von einer entsprechenden Anwendung. Dabei soll festgestellt werden, wo die Befragten die Zuständigkeit für das Betreiben, Instandhalten und die Qualitätsprüfung der Anwendung verorten (z.B. Staatliche Behörden, Social-Media-Plattformen,...). Beispiel: "Aber für die für die Implementation und für die Umsetzung sollten dann halt die Betreiber der verschiedenen Webseiten dann selbstverantwortlich sein." (B2_1, Zeile 134-136). @tbl-qualitative

Des Weiteren beschreibt das Thema "Darstellung und Abruf" die Anforderungen seitens der Befragten an die Kennzeichnung von Misinformation, bspw. durch die Verwendung entsprechender Symbole oder Farben. Hierbei soll auch die richtige Balance bei der Präsenz und Prägnanz der Darstellung gefunden werden: "Ich finde es sollte nicht aufdringlich sein, dementsprechend auch abschaltbar und es sollte sowohl optische als auch sozusagen faktisch nachprüfbare Anteile haben." (B2_2, Zeile 52-54). @tbl-qualitative

Außerdem ist die Transparenz und Quellenangabe seitens der KI bei ihrer Entscheidungsfindung von Relevanz. Ziel hiervon ist es, den Nutzenden das Nachvollziehen der Beurteilung von Informationen mithilfe entsprechender Begründungen und Angaben von Quellen zu ermöglichen: "Also ich würde mir das so vorstellen, dass wenn es zu einer Entscheidung kommt oder zu einer Einschätzung kommt, dass es dann auch dementsprechend Quellenangaben machen kann. Also dass man dann ganz klar sieht, ok, warum hat es sich jetzt dafür entschieden und wie ist es zu diesem Schluss gekommen." (B2_1, Zeile 142-146). @tbl-qualitative

| Thema | Definition | Textstelle |
|------------------------|------------------------|------------------------|
| Verantwortlichkeit und Regulierung | Die Frage nach der Verantwortlichkeit für Betreiben und Regulieren eines solchen KI-Systems (bspw. Plattformbetreiber, Staat oder Aufsichtsbehörden), insbesondere in Fällen von Versagen. | "Aber für die für die Implementation und für die Umsetzung sollten dann halt die Betreiber der verschiedenen Webseiten dann selbstverantwortlich sein." (B2_1, Zeile 134-136) |
| Darstellung und Abruf | Die Anforderungen an die Kennzeichnung von Misinformation, z. B. durch Farbcodes oder kleine Symbole, sowie eine nicht-aufdringliche Integration der KI in die Plattform. | "Ich finde es sollte nicht aufdringlich sein, dementsprechend auch abschaltbar und es sollte sowohl optische als auch sozusagen faktisch nachprüfbare Anteile haben." (B2_2, Zeile 52-54) |
| Transparenz und Quellenangabe | Die Anforderungen an die Nachvollziehbarkeit von KI-Entscheidungen, etwa durch transparente Quellenangaben, eine klare Darstellung des Prüfprozesses und die Offenlegung der Bewertungsgrundlagen. | "Also ich würde mir das so vorstellen, dass wenn es zu einer Entscheidung kommt oder zu einer Einschätzung kommt, dass es dann auch dementsprechend Quellenangaben machen kann. Also dass man dann ganz klar sieht, ok, warum hat es sich jetzt dafür entschieden und wie ist es zu diesem Schluss gekommen." (B2_1, Zeile 142-146) |

: Themen {#tbl-qualitative}

## Quantitative Ergebnisse

**Hinweis auf Einsatz von KI:** Zur Aufbereitung der Daten und Berechnung des t-Tests, der Korrelation und der deskriptiven Statistik mithilfe von R wurde eine generative künstliche Intelligenz eingesetzt, die Teile des Codes bereitgestellt hat.

### Stichprobe

Die vorliegende Stichprobe wurde im Rahmen einer Online-Befragung erhoben, die über einen Web-Link verteilt wurde. Der Erhebungszeitraum erstreckte sich dabei vom 31.05.2025 bis zum 09.06.2025. Insgesamt nahmen 176 Personen an der Befragung teil. Die Zusammensetzung der Stichprobe weist einen Anteil von 51% weiblichen und 48% männlichen Teilnehmern auf. 1% gab an, keinem der Geschlechter zuzugehören. Der Altersdurchschnitt der Stichprobe betrögt dabei 26 Jahre mit einer Standardabweichung von 11. Bezüglich des Bildungsniveaus gaben 90% der Teilnehmenden an, einen Abschluss auf Niveau 5 (Abitur oder ähnliche) zu besitzen, während 7% einen Abschluss auf Niveau 4 (Mittlerer Schulabschluss) vorweisen konnten. Die Selbsteinschätzung der Teilnehmenden hinsichtlich ihrer Kenntnisse über Künstliche Intelligenz variierte auf einer Skala von 1 (am wenigsten) bis 5 (am meisten). Die durchschnittliche Bearbeitungszeit des Fragebogens lag bei 26 Minuten.

Die Datenerhebung erfolgte anonym und freiwillig. Zudem wurden keine personenbezogenen Daten erhoben, die eine Identifizierung ermöglichen würden. Ziel der Befragung war es, ein breites Meinungsbild zu verschiedenen Aspekten rund um digitale Mediennutzung und technologische Kompetenzen im Bezug auf KI und Missinformationen im Internet zu erfassen. Die Ergebnisse der Studie sollen einen Beitrag zum besseren Verständnis digitaler Verhaltensweisen liefern.

### Deskriptive Statistik

Untersucht wird zunächst, inwiefern die Entscheidungssicherheit der Probanden bei der Beurteilung diverser Posts von der Verwendung einer KI beeinflusst wird. Zugrunde liegt die Hypothese, die Zuhilfenahme von KI-Systemen könnte das Vertrauen der Probanden in die eigene Beurteilung von (Mis-)Informationen erhöhen.

Die Durchführung dieser Untersuchung erfolgt in Form eines gepaarten t-Tests, der die Mittelwerte der Variable zur Beschreibung der Entscheidungssicherheit, confidence, aus den entsprechenden Systemtypen vergleicht (@fig-mstbl). Der Systemtyp wird hierbei in der unabhängigen Variable system abgebildet, wobei im Rahmen dieser Untersuchung\* nur zwischen den Systemtypen Baseline (ohne KI) und Evaluative/Recommendative (mit KI) differenziert wird (@fig-cstbl). Gewählt wird ein Signifikanzniveau von 0.05. Das Ergebnis entspricht einem p-Wert von 0.0487435. Somit wird das Ergebnis als statistisch signifikant interpretiert, wodurch unsere Hypothese bestätigt wird.

Des Weiteren wird mittels einer Korrelationsanalyse ein möglicher Zusammenhang der Vorerfahrung mit KI seitens der Probanden mit der benötigten Bearbeitungszeit bei der Verwendung der KI geprüft. Die Variable aiknowledge aus dem Pre-Test-Fragebogen bildet hierbei metrisch auf einer self-designed Skala von 1-5 das angegebene KI-Vorwissen der Probanden ab (@fig-mstbl). Die Bearbeitungszeit im Rahmen der Performanzdatenerhebung wird in der Variable time_on_task gespeichert (@fig-mstbl), wobei für diese Analyse nur die Bearbeitungszeit bei Verwendung einer der beiden KI-Systemtypen berücksichtigt wird.

![](categorial_table.png){#fig-cstbl .figure .quarto-figure fig-cap=""}

![](metric_table.png){#fig-mstbl .figure .quarto-figure fig-cap=""}

### Inferenzstatistik

Um die Hypothese zu überprüfen, ob der Einsatz eines KI-Systems die Entscheidungssicherheit bei der Erkennung von Missinformationen verbessert, haben wir den geplanten t-Test durchgeführt. Dabei wurden die Mittelwerte der Entscheidungssicherheit in der Baseline-Bedingungen (ohne KI) und der KI-Bedingungen miteinander vergleichen. Durch Einsatz der KI (*M* = 4.3, *SD* = 0.8) erzielten die Teilnehmenden eine höhere Entscheidungssicherheit als ohne KI (*M* = 4.0, *SD* = 0.9). Dieser Unterschied ist signifikant (*t*(177) = -6.04, *p* \< .001, *d* = 0.45). Dies deutet darauf hin, dass KI-Empfehlungen dazu beitragen können, das Vertrauen in die eigene Urteilsfähigkeit zu erhöhen (@fig-tTest).

Weiterhin möchten wir die Hypothese überprüfen, die eine kürzere Bearbeitungsdauer bei höherem KI-Vorwissen verspricht. Zwischen Vorerfahrung mit KI und durchschnittlicher Bearbeitungszeit besteht kein signifikanter Zusammenhang (*r*(167) = .05, *p* = .529). Das bedeutet, dass ein höheres KI-Vorwissen nicht mit einer kürzeren Bearbeitungsdauer einherging (@fig-corr).

```{r fig-tTest, fig.cap="Mittelwertvergleich Confidence (Baseline vs. KI)", echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(psych)
library(knitr)
library(ggplot2)
library(tidyr)
library(kableExtra)

data_combined <- read.csv("data_combined.csv")
web_app_data <- read.csv("web_app_data.csv")


#Filtern und Gruppieren:

filtered_data_combined <- data_combined %>%
  filter(age > 18 & age < 120) 

datengesamt <- filtered_data_combined %>% full_join(web_app_data, by = "participantId")

daten_clean <- datengesamt %>%
  filter(!is.na(confidence), !is.na(system), !is.na(time_on_task), !is.na(aiknowledge))

baseline_means <- datengesamt %>%
  filter(system == "B") %>%                
  group_by(participantId) %>%                     
  summarise(mean_confidence_baseline = mean(confidence, na.rm = TRUE))   

ki_means <- datengesamt %>%
  filter(system %in% c("E", "R")) %>%  
  group_by(participantId) %>%
  summarise(mean_confidence_ki = mean(confidence, na.rm = TRUE))

combined <- baseline_means %>%
  full_join(ki_means, by = "participantId")


t_test <- t.test(combined$mean_confidence_baseline, combined$mean_confidence_ki, paired = TRUE)

plot_data <- combined %>%
  pivot_longer(
    cols = c(mean_confidence_baseline, mean_confidence_ki),
    names_to = "group",
    values_to = "mean_confidence"
  ) %>%
  mutate(group = recode(group,
                        mean_confidence_baseline = "Baseline",
                        mean_confidence_ki = "KI"))

plot_summary <- plot_data %>%
  group_by(group) %>%
  summarise(
    mean = mean(mean_confidence, na.rm = TRUE),
    sd = sd(mean_confidence, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n),
    ci_lower = mean - qt(0.975, df = n - 1) * se,
    ci_upper = mean + qt(0.975, df = n - 1) * se,
    .groups = "drop"
  )


ggplot(plot_summary, aes(x = group, y = mean)) +
  geom_point(size = 4, color = "#2C3E50") +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.1, color = "#2C3E50") +
  labs(
    x = "Bedingung",
    y = "Mittlere Confidence"
  ) +
  theme_minimal(base_size = 14)
```

```{r fig-corr, fig.cap="Korrelation zwischen Zeitaufwand und KI-Wissen", echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(psych)
library(knitr)
library(ggplot2)
library(tidyr)
library(kableExtra)

data_combined <- read.csv("data_combined.csv")
web_app_data <- read.csv("web_app_data.csv")


#Filtern und Gruppieren:

filtered_data_combined <- data_combined %>%
  filter(age > 18 & age < 120) 

#Korrelation:


time_on_task_means <- filtered_data_combined[, c("aiknowledge", "time_on_task_mean_E", "time_on_task_mean_R")]

time_on_task_means <- time_on_task_means %>%
  mutate(
    time_on_task_mean = as.numeric(coalesce(time_on_task_mean_E, time_on_task_mean_R)),
    aiknowledge = as.numeric(aiknowledge)
  )


ggplot(time_on_task_means, aes(x = time_on_task_mean, y = aiknowledge)) +
  geom_point(color = "steelblue", size = 2) +
  geom_smooth(method = "lm", se = TRUE, color = "darkred") +
  labs(
    x = "Durchschnittliche Bearbeitungszeit",
    y = "KI-Wissen (aiknowledge)"
  ) +
  theme_minimal(base_size = 14)
```

## 5. Diskussion

### Bedeutung der Arbeit für die Forschungsfrage

#### Ziel

Die vorliegende Arbeit widmet sich der zentralen Fragestellung, inwiefern Künstliche Intelligenz zur Unterstützung beim Filtern von Misinformationen in sozialen Medien beitragen kann. Diese Frage gewinnt angesichts der wachsenden Bedeutung von Social Media als primäre Informationsquelle für viele Menschen zunehmend an Relevanz. Durch verschiedene Methoden haben wir versucht uns der Frage zu stellen.

#### Methoden

Zur Beantwortung der Forschungsfrage wurde eine qualitative Analyse ausgeführt welche wir mit quantitativen Ansätzen weiter angeführt haben.

In der qualitativen Analyse haben wir uns für ein Interview entschieden mit 2 Teilnehmenden, die in unserem Studiengang sind und somit möglicherweise entsprechende Erfahrungen zu KI haben.

Innerhalb der quantitativen Datenerhebung wurden zur Teilnahme an der Erhebung den Probanden ein Link zur Verfügung gestellt, über den sie eigenständig teilnehmen konnten.

Dabei handelte es sich bei der Erhebung um die Untersuchung des Effekts von KI als Unterstützung bei der Detektion von Misinformationen wobei die Teilnehmenden zunächst Beiträge ohne KI bewerten sollten und anschließend mit KI (Within-Subjects-Design).

#### Ergebnis

Aus der quantitativen Methode stellte sich unter anderen heraus dass, durch Einsatz der KI die Teilnehmenden eine höhere Entscheidungssicherheit als ohne KI erzielten. Dieser Unterschied ist signifikant. Dies deutet darauf hin, dass KI-Empfehlungen dazu beitragen können, das Vertrauen in die eigene Urteilsfähigkeit zu erhöhen.

Aus der qualitativen Methode ergab sich, dass Betreiber der verschiedenen Webseiten  selbstverantwortlich sein sollten, es faktisch nachprüfbare Anteile haben muss und, dass es auch dementsprechend Quellenangaben geben soll.

#### Explizite Beantwortung der Forschungsfrage

Die Arbeit kommt zu dem Ergebnis, dass KI ein wirksames Werkzeug zur Unterstützung beim Filtern von Misinformationen in sozialen Medien darstellen kann, sofern sie gezielt eingesetzt und mit menschlicher Aufsicht kombiniert wird.

### Implikationen für die Gesellschaft

In den Ergebnissen unserer Studie können wir Rückschlüsse auf gesellschaftliche Phänomene im Zusammenhang mit der Handhabung von digitalen Informationen ziehen. Konkret heißt das, dass KI-Empfehlungen die Entscheidungssicherheit von Nutzenden in sozialen Netzwerken signifikant erhöhen. Diese Information ist wertvoll, um in der zunehmend komplexen Informationslandschaft, die ein hohes Risiko für Misinformation birgt, immer bessere und sinnvollere Systeme zu entwickeln, die den Menschen bei der Orientierung auf digitalen Medien, insbesondere sozialen Medien, unterstützt. Wichtig ist zudem die Erkenntnis, dass die KI-Unterstützung vor allem dann angenommen wird, wenn diese verständlich und glaubwürdig präsentiert wird. Ebenfalls lässt sich aus den Interviews schließen, dass eine hohe Sorgfalt bei der Transparenz und Quellenangabe bei etwaigen KI-Systemen erwartet wird. Dies sollten Plattformbetreiber und andere verantwortliche Institutionen bei der Entwicklung zukünftiger KI-Assistenzen beachten. Wie Nutzende auf diese reagieren und was sie erwarten, ist auch für die Politik interessant, die in Regulationen für den Einsatz von KI jeglicher Art schaffen muss und den Bedarf an Kennzeichnung von KI, Quellenoffenlegung und ethischen Risiken so besser feststellen kann.

### Implikationen für die Wissenschaft

Fortführend können wir feststellen, dass unsere Arbeit auch eine Relevanz in der wissenschaftlichen Diskussion um Mensch-KI-Interaktion, Vertrauen und Medienkompetenz hat. Erkenntnisse aus einer anderen Studie [@jahanbakhsh2023], die darauf hinweist, dass personalisierte KI-Systeme die Urteilsbildung beeinflussen, können wir bestätigen. Auch hier war die Transparenz und Nachvollziehbarkeit der KI stark gewichtet. Was uns zudem auffällt, ist, dass nach unserem Korrelationstest kein weiterer Zusammenhang zwischen technikbezogener Vorerfahrung (ATI) und der Wirkung der KI-Empfehlung gefunden wurde. Dies ist ein Widerspruch zu bisherigen Annahmen, die technikaffinen Menschen eine hohe Akzeptanz und einen hohen Nutzungsgrad von KI-Systemen nahelegt [@franke2019].

### Limitationen

#### Quantitative Methode

Die Teilnehmenden unserer Studie wurden überwiegend über eine Online-Verbreitung in einem universitären Umfeld rekrutiert. Diese Art der Rekrutierung kann zu einer Selbstselektion der Teilnehmenden geführt haben, welche eventuell ein besonderes Interesse an KI oder digitaler Mediennutzung haben. Hinzukommt, dass der größte Teil unserer Teilnehmenden angab, einen höheren Bildungsabschluss zu besitzen, wobei der Altersdurchschnitt bei 26 Jahren lag. Diese vorangehenden Punkte zeigen, dass unsere Stichprobe deshalb nicht als repräsentativ für die Gesamtbevölkerung angesehen werden kann.

Bei der Frage zur Vorerfahrung mit KI könnten Teilnehmende aufgrund des aktuell im Trend liegenden KI-Themas zu sozial erwünschten Antworten tendiert haben und somit ihre eigenen Fähigkeiten überschätzt haben.

#### Qualitative Methode

Bei der qualitativen Erhebung wurden nur zwei Teilnehmende interviewt. Dies ist eine kleine Stichprobengröße für die gewählte Analysemethode. Außerdem wurde ein Interview online und ein Interview in Person durchgeführt. Hierbei wurden unterschiedliche Aufnahmemethoden (Smartphone-Mikrofon vs. OBS) verwendet. Somit könnten mögliche Unterschiede in der Aufnahmequalität und Gesprächsdynamik zu unterschiedlichen Analyseergebnissen geführt haben. Insgesamt sollten unsere Ergebnisse daher mehr als Trend, als eine repräsentative Meinungsabbildung angesehen werden.

### Möglichkeiten für zukünftige Arbeiten

Künftige Studien könnten verschiedene Nutzergruppen (z. B. Laien vs. Expert\*innen, verschiedene Altersgruppen oder politische Einstellungen) gezielt vergleichen, um zu analysieren, unter welchen Bedingungen KI-Empfehlungen tatsächlich als hilfreich, vertrauenswürdig oder vielleicht auch als verzerrend wahrgenommen werden. Ein weiterer Ansatz ergibt sich aus einem Ergebnis der qualitativen Analyse: die Möglichkeit nach Nachprüfbarkeit und Quellenangaben. In zukünftigen Studien könnte gezielt untersucht werden, welchen Einfluss transparente Erklärungen der KI (z. B. Quellenangaben, Begründungen für eine Bewertung) auf das Vertrauen der Nutzenden haben. Für eine künftige Studie könnte ein Between-Subjects-Design gewählt werden. Dabei würden unterschiedliche Gruppen jeweils nur mit oder nur ohne KI arbeiten, um so mögliche Unterschiede im Vergleich besser sichtbar zu machen. Dadurch ließen sich Interpretationen und ob die KI wirklich unterstützt, weiter aufklären. Der Einsatz von KI zur Unterstützung bei der Erkennung von Misinformationen in Social Media bleibt ein spannendes Forschungsgebiet und hat wichtige Erkenntnisse gebracht. Durch verschiedene Forschungsfragen und Methoden lässt das Thema auch noch viel Potential zur Erweiterung für neue Erkenntnisgewinnung offen.

#### Zielgruppenerweiterung

Zukünftige Arbeiten könnten gezielt ältere oder technikferne Zielgruppen einbeziehen, um Unterschiede in der Entscheidungssicherheit zwischen Altersgruppen zu analysieren. Außerdem würde eine Durchführung mit einer repräsentativen Zusammensetzung der Stichprobe hinsichtlich Alter, Beruf und Bildungssabschluss zu verallgemeinerbaren Aussagen führen.

#### Experimentielle Variationen des KI-Systems

Die aus unserer qualitativen Erhebung gewonnenen Anforderungen an ein KI-System hinsichtlich Darstellung und Transparenz könnten in folgenden Arbeiten variiert werden. Hierbei wären Unterschiede in der Art der Darstellung der Quellen (bzw. keine Angabe von Quellen) sowie der Art der Interaktion (dialogbasiert vs. rein informierend über visuelle Marker) von Interesse, um den Einfluss auf Vertrauen und Akzeptanz zu untersuchen.

#### Zeitraumerweiterung

Unsere Erhebung fand über zehn Tage statt. Eine mehrfache Erhebung über einen längeren Zeitraum hinweg kann aufgrund der aktuellen sehr schnellen Entwicklungen im Bereich KI in Betracht gezogen werden. Somit wäre es möglich, Lerneffekte oder Veränderungen im Vertrauen in KI-Systeme zu untersuchen.

## Literaturverzeichnis

::: {#refs}
:::

### Anhang 1 - Rekrutierungstext

Jeden Tag werden unzählbar viele Inhalte auf Social Media veröffentlicht. Jeder kann selbst zum Autor werden und nicht immer ist klar, welche Information tatsächlich wahr ist. So entstehen schnell Misinformationen.\
Um dieses Problem weiter zu untersuchen, möchten wir mit unserer Studie herausfinden, ob eine künstliche Intelligenz dabei helfen kann, Misinformationen zu identifzieren und entsprechend zu kennzeichnen. Du sollst hierbei einmal ohne und einmal mit Empfehlung der KI entscheiden, ob ein Post Misinformation darstellt oder nicht.

Voraussetzungen

-   Mindestens 18 Jahre alt

-   Gute Deutschkenntnisse

-   Zugang zu PC oder Tablet (keine Smartphone-Teilnahme)

-   60 Minuten Zeit

-   Aktuell keine Teilnahme am Modul SMNF

Teilnahmelink:

Link öffnen (Bitte im Browser auf einem PC oder Tablet öffnen.)

VP-Stunden:

Studierende der Medieninformatik oder Psychologie an der Universität zu Lübeck können für die Teilnahme 1 VP-Stunde erhalten.

### Stichprobe

Die vorliegende Stichprobe wurde im Rahmen einer Online-Befragung erhoben, die über einen web Link verteilt wurde. Der Erhebungszeitraum erstreckte sich dabei vom 31.05.2025 bis zum 09.06.2025. Insgesamt nahmen 176 Personen an der Befragung teil. Die Zusammensetzung der Stichprobe weist einen Anteil von \[Prozentzahl\]% weiblichen und \[Prozentzahl\]% männlichen Teilnehmern auf. Der Altersdurchschnitt der Stichprobe betrögt dabei \[Alter\] Jahre mit einer Standardabweichung von \[SD\]. Bezüglich des Bildungsniveaus gaben \[Prozentzahl\]% der Teilnehmenden an, einen \[Art des Abschlusses\] zu besitzen, während \[Prozentzahl\]% einen \[anderer Abschluss\] vorweisen konnten. Die Selbsteinschätzung der Teilnehmenden hinsichtlich ihrer Kenntnisse über Künstliche Intelligenz variierte auf einer Skala von \[min. Wert\] bis \[max. Wert\]. Die durchschnittliche Bearbeitungszeit des Fragebogens lag bei \[Zeitangabe\] Minuten.

Die Datenerhebung erfolgte anonym und freiwillig. Zudem wurden keine personenbezogenen Daten erhoben, die eine Identifizierung ermöglichen würden. Ziel der Befragung war es, ein breites Meinungsbild zu verschiedenen Aspekten rund um digitale Mediennutzung und technologische Kompetenzen im Bezug auf KI und Missinformationen im Internet zu erfassen. Die Ergebnisse der Studie sollen einen Beitrag zum besseren Verständnis digitaler Verhaltensweisen liefern.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(psych)
library(knitr)
library(ggplot2)
library(tidyr)
library(kableExtra)

data_combined <- read.csv("data_combined.csv")
web_app_data <- read.csv("web_app_data.csv")


#Filtern und Gruppieren:

filtered_data_combined <- data_combined %>%
  filter(age > 18 & age < 120) 

summary <- filtered_data_combined %>%
  filter(gender == "female")
  
datengesamt <- filtered_data_combined %>% full_join(web_app_data, by = "participantId")

daten_clean <- datengesamt %>%
  filter(!is.na(confidence), !is.na(system), !is.na(time_on_task), !is.na(aiknowledge))






#Metrische Werte und Tabelle:

deskriptiv <- daten_clean %>%
  summarise(
    n_confidence = sum(!is.na(confidence)),
    mean_conf = mean(confidence),
    sd_conf = sd(confidence),
    median_conf = median(confidence),
    min_conf = min(confidence),
    max_conf = max(confidence),
    
    n_time = sum(!is.na(time_on_task)),
    mean_time = mean(time_on_task),
    sd_time = sd(time_on_task),
    median_time = median(time_on_task),
    min_time = min(time_on_task),
    max_time = max(time_on_task),
    
    n_ai = sum(!is.na(aiknowledge)),
    mean_ai = mean(aiknowledge),
    sd_ai = sd(aiknowledge),
    median_ai = median(aiknowledge),
    min_ai = min(aiknowledge),
    max_ai = max(aiknowledge)
  )


variablen <- c("confidence", "time_on_task", "aiknowledge")

n <- c(deskriptiv$n_confidence, deskriptiv$n_time, deskriptiv$n_ai)
mean <- c(deskriptiv$mean_conf, deskriptiv$mean_time, deskriptiv$mean_ai)
sd <- c(deskriptiv$sd_conf, deskriptiv$sd_time, deskriptiv$sd_ai)
median <- c(deskriptiv$median_conf, deskriptiv$median_time, deskriptiv$median_ai)
min <- c(deskriptiv$min_conf, deskriptiv$min_time, deskriptiv$min_ai)
max <- c(deskriptiv$max_conf, deskriptiv$max_time, deskriptiv$max_ai)

deskriptiv_tabelle <- data.frame(
  Variable = variablen,
  n = n,
  mean = round(mean, 2),
  sd = round(sd, 2),
  median = round(median, 2),
  min = min,
  max = max
)


#deskriptiv_tabelle %>%
#  kable("html", caption = "Metrische Statistiken", digits = 2) %>%
#  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "center")














#Kategoriale Werte und Tabelle:


abs_haeufigkeit <- table(daten_clean$system)
rel_haeufigkeit <- prop.table(table(daten_clean$system)) * 100

metrische_variablen <- daten_clean %>%
  select(confidence, time_on_task, aiknowledge)


metrische_statistik <- describe(metrische_variablen)[, c("n", "mean", "median", "min", "max", "sd")]

metrische_statistik <- round(metrische_statistik, 2)

kategoriale_tabelle <- data.frame(
  System = names(abs_haeufigkeit),
  Anzahl = as.vector(abs_haeufigkeit),
  Prozent = round(as.vector(rel_haeufigkeit), 1)
)


#kategoriale_tabelle %>%
#  kable("html", caption = "Kategoriale Statistiken", digits = 2) %>%
#  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "center")















#T-Test:

baseline_means <- datengesamt %>%
  filter(system == "B") %>%                
  group_by(participantId) %>%                     
  summarise(mean_confidence_baseline = mean(confidence, na.rm = TRUE))   

ki_means <- datengesamt %>%
  filter(system %in% c("E", "R")) %>%  
  group_by(participantId) %>%
  summarise(mean_confidence_ki = mean(confidence, na.rm = TRUE))

combined <- baseline_means %>%
  full_join(ki_means, by = "participantId")


t_test <- t.test(combined$mean_confidence_baseline, combined$mean_confidence_ki, paired = TRUE)


plot_data <- combined %>%
  pivot_longer(
    cols = c(mean_confidence_baseline, mean_confidence_ki),
    names_to = "group",
    values_to = "mean_confidence"
  ) %>%
  mutate(group = recode(group,
                        mean_confidence_baseline = "Baseline",
                        mean_confidence_ki = "KI"))

plot_summary <- plot_data %>%
  group_by(group) %>%
  summarise(
    mean = mean(mean_confidence, na.rm = TRUE),
    sd = sd(mean_confidence, na.rm = TRUE),
    n = n(),
    se = sd / sqrt(n),
    ci_lower = mean - qt(0.975, df = n - 1) * se,
    ci_upper = mean + qt(0.975, df = n - 1) * se,
    .groups = "drop"
  )

  


#Korrelation:


time_on_task_means <- filtered_data_combined[, c("aiknowledge", "time_on_task_mean_E", "time_on_task_mean_R")]

time_on_task_means <- time_on_task_means %>%
  mutate(
    time_on_task_mean = as.numeric(coalesce(time_on_task_mean_E, time_on_task_mean_R)),
    aiknowledge = as.numeric(aiknowledge)
  )






```
